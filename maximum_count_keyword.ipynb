{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDuyqs3KqiSGd8IOU9UuY9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anushaacharya607/Anushaacharya607.github.io/blob/master/maximum_count_keyword.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General Strategy:\n",
        "1.   Clean Data and tokenize\n",
        "2.   Seperate korean and romans\n",
        "3.   romans:Identify indonesia--> search in pickle, get lemma, get pos_tag\n",
        "4.   else: pass through english spacy; get lemma; get pos_tag\n",
        "5.   get korean pos_tag\n",
        "6.   pass through individual stopwords\n",
        "7.   get high freq words with pos_tag nn or jj\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SOatQQnn1MvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hangul seperator\n",
        "!pip install hangul-jamo\n",
        "\n",
        "# Lemmatizer\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download ko_core_news_sm\n",
        "# load indo pickle\n",
        "!pip install backports.lzma\n",
        "\n",
        "!pip install wget\n"
      ],
      "metadata": {
        "id": "e7Q6VBSnRk37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# cleaning and tokenization\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Hangul seperator\n",
        "import hangul_jamo\n",
        "\n",
        "# Lemmatizer\n",
        "import spacy\n",
        "spe = spacy.load('en_core_web_sm')\n",
        "spk = spacy.load('ko_core_news_sm')\n",
        "\n",
        "#pickel\n",
        "import pickle\n",
        "try:\n",
        "    import lzma\n",
        "except ImportError:\n",
        "    from backports import lzma\n",
        "    \n",
        "import wget\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "35OgNKZ64g2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_cleaning(text):\n",
        "  tokens = []\n",
        "  # Tokenization\n",
        "  TOKREGEX = re.compile(\n",
        "    r'(?:(?:[0-9][0-9.,:%-]*|St\\.)[\\w_€-]+|https?://[^ ]+|[@#§$]?\\w[\\w*_-]*|[,;:\\.?!¿¡‽⸮…()\\[\\]–{}—―/‒_“„”⹂‚‘’‛′″‟\\'\"«»‹›<>=+−×÷•·]+)'\n",
        "    )\n",
        "  tokens = TOKREGEX.findall(text)\n",
        "  tokens = list(filter(lambda token: token not in string.punctuation, tokens))\n",
        "  # Double cleaning\n",
        "  for token in tokens:\n",
        "    for x in string.punctuation:\n",
        "      if token.__contains__(x):\n",
        "        tokens.remove(token)\n",
        "        break\n",
        "  # Lower\n",
        "  tokens = list(map(lambda token: token.lower(), tokens))\n",
        "  return tokens\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2bEGB0NY1MCs"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iswordKorean(word):\n",
        "  characters = hangul_jamo.decompose(word)\n",
        "  for character in characters:\n",
        "    if not hangul_jamo.is_jamo_character(character):return False\n",
        "  return True"
      ],
      "metadata": {
        "id": "ljDy9uTYAusw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iswordID(word):\n",
        "  stopwordsID = stopWordsID()\n",
        "  id_dict =_load_pickle()\n",
        "  if word in stopwordsID: return True\n",
        "  elif id_dict.get(word): return True\n",
        "  else: return False"
      ],
      "metadata": {
        "id": "KwCwJ0tagsL8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopWordsID():\n",
        "  filehandle = open(\"/content/StopwordsID.txt\", \"r\")\n",
        "  data = filehandle.read()\n",
        "  stopwords = data.split('\\n')\n",
        "  filehandle.close()\n",
        "  return stopwords"
      ],
      "metadata": {
        "id": "QNQOcxvcxpI3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.util import working_dir\n",
        "def lemmatizer(tokens):\n",
        "  rows = []\n",
        "  cols = (\"text\", \"lemma\", \"POS\",\"Stop Words\")\n",
        "  id_dict =_load_pickle()\n",
        "  stopwords_ko = spk.Defaults.stop_words\n",
        "  stopwords_en = spe.Defaults.stop_words\n",
        "  for token in tokens:\n",
        "    if iswordKorean(token):\n",
        "      word = spk(token)\n",
        "      for k in word:\n",
        "        lemma = k.lemma_.split('+', 1)[0]\n",
        "        pos = k.pos_\n",
        "        if lemma in stopwords_ko:row = [k, lemma, pos, 1]\n",
        "        else: row = [k, lemma, pos, 0]\n",
        "    elif iswordID(token):\n",
        "      ID_pos = ID_pos_()\n",
        "      if token in ID_pos.values:\n",
        "        pos = ID_pos.loc[ID_pos[0]==token].iloc[0][1]\n",
        "      else: pos = 'NN'\n",
        "      if token in stopWordsID():row=[token, token, pos, 1]\n",
        "      elif id_dict.get(token): row=[token, id_dict.get(token), pos, 0]\n",
        "  \n",
        "    else:\n",
        "      word = spe(token)\n",
        "      for o in word:\n",
        "        lemma = o.lemma_.split('+', 1)[0]\n",
        "        pos = o.pos_\n",
        "        if lemma.lower() in stopwords_en: row = [o, lemma, pos, 1]\n",
        "        else:row = [o, lemma, pos, 0]\n",
        "    rows.append(row)\n",
        "  \n",
        "  return pd.DataFrame(rows, columns=cols)\n",
        "\n"
      ],
      "metadata": {
        "id": "0-Qxr9r1GeEG"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_pickle():\n",
        "  filepath = str(\"/content/id.plzma\") \n",
        "  with lzma.open(filepath, \"rb\") as filehandle:\n",
        "    pickled_dict = pickle.load(filehandle)\n",
        "    assert isinstance(pickled_dict, dict)\n",
        "    return pickled_dict"
      ],
      "metadata": {
        "id": "Tnjq5w_5VgVu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ID_pos_():\n",
        "  url =\"https://raw.githubusercontent.com/famrashel/idn-tagged-corpus/master/Indonesian_Manually_Tagged_Corpus.tsv\"\n",
        "  Id_tag_corpus = wget.download(url)\n",
        "  ID_pos=pd.read_csv(Id_tag_corpus,sep='\\t', header=None)\n",
        "  return ID_pos\n"
      ],
      "metadata": {
        "id": "Euz8tAgZN6z1"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _Keyword(text, freq_limit=1, POS=['NOUN', 'NN'], num=None):\n",
        "  filtered = pre_cleaning(text)\n",
        "  df = lemmatizer(filtered)\n",
        "  cd = df.loc[(df['Stop Words']== 0) & (df['POS'].isin(POS))]\n",
        "  c = Counter({k: c for k, c in Counter(cd[\"lemma\"]).items() if c > freq_limit})\n",
        "  return c.most_common(num)  "
      ],
      "metadata": {
        "id": "boLR3dhCLK-F"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''' I am Anusha eating and again eating rice and I think I will go to fishing today and catch some fish. She loves it.\n",
        "밥을 먹고 오늘은 낚시를 하러 갈 오늘 갈 것 같다.\n",
        "Saya sedang makan nasi dan saya pikir saya akan pergi memancing hari ini.\n",
        "I am eating and again eating rice and I think I will go to fishing today and catch some fish. She loves it.\n",
        "밥을 먹고 오늘은 낚시를 하러 갈 오늘 갈 것 같다.\n",
        "Saya sedang makan nasi dan saya pikir saya akan pergi memancing hari ini.'''\n",
        "_Keyword(text, freq_limit=1, POS=['NOUN', 'NN', 'VB', 'VERB'] ) #num optional argument"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A_Gbyu-LzVz",
        "outputId": "a0dc780b-0606-4516-8304-bef56ec9dd3f"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('eat', 4),\n",
              " ('오늘', 4),\n",
              " ('rice', 2),\n",
              " ('think', 2),\n",
              " ('fishing', 2),\n",
              " ('today', 2),\n",
              " ('catch', 2),\n",
              " ('fish', 2),\n",
              " ('love', 2),\n",
              " ('밥', 2),\n",
              " ('낚시', 2),\n",
              " ('makan', 2),\n",
              " ('nasi', 2),\n",
              " ('pikir', 2),\n",
              " ('pergi', 2),\n",
              " ('pancing', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    }
  ]
}